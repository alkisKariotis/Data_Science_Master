{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "013f3146-548b-458f-874a-27df0ca77b5d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Alkiviadis Kariotis 241735 HW#2 \n",
    "ITC6010A1 - NATURAL LANGUAGE PROCESSING - Spring Term 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde597ac-3ad8-4433-b268-dd071559d673",
   "metadata": {},
   "source": [
    "## Trigram Language Model using the Reuters dataset.Evaluation of the model and reporting its perplexity for different cases of smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8130f1e-3415-4757-a712-fc173f9dfe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the model on training data:  6.020840213995279\n",
      "Perplexity of the model on test data:  132854.35510278275\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize the count dictionaries and total_words variable\n",
    "        self.unigram_counts = collections.defaultdict(int)\n",
    "        self.bigram_counts = collections.defaultdict(int)\n",
    "        self.trigram_counts = collections.defaultdict(int)\n",
    "        self.total_words = 0\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Training the language model on the given corpus\n",
    "        for sentence in corpus:\n",
    "            # Update the counts for unigrams, bigrams, and trigrams\n",
    "            for unigram in sentence:\n",
    "                self.unigram_counts[unigram] += 1\n",
    "                self.total_words += 1\n",
    "            for bigram in bigrams(sentence, pad_left=True, pad_right=True):\n",
    "                self.bigram_counts[bigram] += 1\n",
    "            for trigram in trigrams(sentence, pad_left=True, pad_right=True):\n",
    "                self.trigram_counts[trigram] += 1\n",
    "\n",
    "    def score(self, sentence):\n",
    "        # Calculate the log probability score for a given sentence\n",
    "        score = 0.0\n",
    "        for trigram in trigrams(sentence, pad_left=True, pad_right=True):\n",
    "            if self.bigram_counts[trigram[:2]] > 0:\n",
    "                trigram_count = self.trigram_counts[trigram]\n",
    "                if trigram_count > 0:\n",
    "                    score += math.log(trigram_count)\n",
    "                else:\n",
    "                    score += math.log(1e-10)  # assign a very small probability for unseen trigrams\n",
    "                score -= math.log(self.bigram_counts[trigram[:2]])\n",
    "            else:\n",
    "                unigram_count = self.unigram_counts[trigram[2]]\n",
    "                if unigram_count > 0:\n",
    "                    score += math.log(unigram_count + 1)\n",
    "                else:\n",
    "                    score += math.log(1e-10)  # assign a very small probability for unseen unigrams\n",
    "                score -= math.log(self.total_words + len(self.unigram_counts))\n",
    "        return score\n",
    "\n",
    "\n",
    "    def perplexity(self, corpus):\n",
    "        # Calculate the perplexity of the model on a given corpus\n",
    "        total_log_prob = 0\n",
    "        test_words = 0\n",
    "        for sentence in corpus:\n",
    "            total_log_prob += self.score(sentence)\n",
    "            test_words += len(sentence)\n",
    "        return math.exp(-total_log_prob / test_words)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Download necessary NLTK resources\n",
    "    nltk.download('reuters')\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Prepare train and test corpus from the Reuters dataset\n",
    "    train_corpus = [nltk.word_tokenize(reuters.raw(fileid)) for fileid in reuters.fileids() if fileid.startswith('training/')]\n",
    "    test_corpus = [nltk.word_tokenize(reuters.raw(fileid)) for fileid in reuters.fileids() if fileid.startswith('test/')]\n",
    "\n",
    "    # Create an instance of TrigramLanguageModel\n",
    "    model = TrigramLanguageModel()\n",
    "\n",
    "    # Train the model on the training corpus\n",
    "    model.train(train_corpus)\n",
    "\n",
    "    # Calculate and print the perplexity of the model on training and test data\n",
    "    print(\"Perplexity of the model on training data: \", model.perplexity(train_corpus))\n",
    "    print(\"Perplexity of the model on test data: \", model.perplexity(test_corpus))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bb0f1-e8cc-4c8a-813f-6bff96f23b4e",
   "metadata": {},
   "source": [
    "## Spell checker using the Trigram Language Model and the Noisy Channel Approach. Proposing candidates for correction based on both Edit Distance and the probability given by the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d22780-b392-4b7a-88ba-390eeba25d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  This is a tst sentnce .\n",
      "Corrected sentence:  This is a test sentence .\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unigram_counts = collections.defaultdict(int)  # Counts of individual words (unigrams)\n",
    "        self.bigram_counts = collections.defaultdict(int)  # Counts of consecutive word pairs (bigrams)\n",
    "        self.trigram_counts = collections.defaultdict(int)  # Counts of three consecutive words (trigrams)\n",
    "        self.total_words = 0  # Total number of words in the training corpus\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            for unigram in sentence:\n",
    "                self.unigram_counts[unigram] += 1\n",
    "                self.total_words += 1\n",
    "            for bigram in bigrams(sentence, pad_left=True, pad_right=True):\n",
    "                self.bigram_counts[bigram] += 1\n",
    "            for trigram in trigrams(sentence, pad_left=True, pad_right=True):\n",
    "                self.trigram_counts[trigram] += 1\n",
    "\n",
    "    def score(self, sentence):\n",
    "        score = 0.0\n",
    "        for trigram in trigrams(sentence, pad_left=True, pad_right=True):\n",
    "            if self.bigram_counts[trigram[:2]] > 0:  # Check if the bigram exists in the training data\n",
    "                score += math.log(self.trigram_counts[trigram] + 1)  # Add smoothed trigram count to the score\n",
    "                score -= math.log(self.bigram_counts[trigram[:2]] + len(self.unigram_counts))  # Subtract smoothed bigram count\n",
    "            else:\n",
    "                score += math.log(self.unigram_counts[trigram[2]] + 1)  # If bigram not found, use unigram count as a fallback\n",
    "                score -= math.log(self.total_words + len(self.unigram_counts))  # Subtract total word count\n",
    "        return score\n",
    "\n",
    "    def spell_check(self, sentence):\n",
    "        corrected_sentence = []\n",
    "        for word in sentence:\n",
    "            if word not in self.unigram_counts:  # Check if the word is in the training data\n",
    "                candidates = self.generate_candidates(word)  # Generate candidate corrections for the misspelled word\n",
    "                if candidates:\n",
    "                    corrected_word = max(candidates, key=lambda w: (self.score([w]), -edit_distance(word, w)))  # Choose the best correction based on score and edit distance\n",
    "                    corrected_sentence.append(corrected_word)\n",
    "                else:\n",
    "                    corrected_sentence.append(word)  # If no candidates found, keep the original word\n",
    "            else:\n",
    "                corrected_sentence.append(word)  # If the word is in the training data, keep it as is\n",
    "        return corrected_sentence\n",
    "\n",
    "    def generate_candidates(self, word):\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:] for L, R in splits if R]  # Delete one letter\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]  # Swap adjacent letters\n",
    "        replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]  # Replace a letter with another\n",
    "        inserts    = [L + c + R for L, R in splits for c in letters]  # Insert a letter\n",
    "        return set(deletes + transposes + replaces + inserts)  # Return a set of unique candidate corrections\n",
    "\n",
    "def main():\n",
    "    nltk.download('reuters')  # Download the Reuters corpus\n",
    "    nltk.download('punkt')  # Download the Punkt tokenizer for tokenization\n",
    "\n",
    "    corpus = [nltk.word_tokenize(reuters.raw(fileid)) for fileid in reuters.fileids() if fileid.startswith('training/')]  # Tokenize and extract sentences from the Reuters training corpus\n",
    "\n",
    "    model = TrigramLanguageModel()  # Create an instance of the TrigramLanguageModel class\n",
    "    model.train(corpus)  # Train the model using the training corpus\n",
    "\n",
    "    test_sentence = ['This', 'is', 'a', 'tst', 'sentnce', '.']\n",
    "    print('Original sentence: ', ' '.join(test_sentence))\n",
    "    corrected_sentence = model.spell_check(test_sentence)  # Perform spell checking on the test sentence\n",
    "    print('Corrected sentence: ', ' '.join(corrected_sentence))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938631e-4d2f-40fa-b09c-b4ca89739761",
   "metadata": {},
   "source": [
    "## Combined both aforementioned procedures for the Gutenberg project corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3a85cf-f30b-4163-a6c6-a0fee3bd09bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of the model on training data:  14860.635906644056\n",
      "Perplexity of the model on test data:  14461.73760696\n",
      "Original sentence:  This is a tst sentnce .\n",
      "Corrected sentence:  This is a tut sentence .\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import requests\n",
    "\n",
    "class TrigramLanguageModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.unigram_counts = collections.defaultdict(int)  # Counts of individual words (unigrams)\n",
    "        self.bigram_counts = collections.defaultdict(int)  # Counts of consecutive word pairs (bigrams)\n",
    "        self.trigram_counts = collections.defaultdict(int)  # Counts of three consecutive words (trigrams)\n",
    "        self.total_words = 0  # Total number of words in the training corpus\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            for unigram in sentence:\n",
    "                self.unigram_counts[unigram] += 1\n",
    "                self.total_words += 1\n",
    "            for bigram in bigrams(sentence, pad_left=True, pad_right=True):\n",
    "                self.bigram_counts[bigram] += 1\n",
    "            for trigram in trigrams(sentence, pad_left=True, pad_right=True):\n",
    "                self.trigram_counts[trigram] += 1\n",
    "\n",
    "    def score(self, sentence):\n",
    "        score = 0.0\n",
    "        for trigram in trigrams(sentence, pad_left=True, pad_right=True):\n",
    "            if self.bigram_counts[trigram[:2]] > 0:  # Check if the bigram exists in the training data\n",
    "                score += math.log(self.trigram_counts[trigram] + 1)  # Add smoothed trigram count to the score\n",
    "                score -= math.log(self.bigram_counts[trigram[:2]] + len(self.unigram_counts))  # Subtract smoothed bigram count\n",
    "            else:\n",
    "                score += math.log(self.unigram_counts[trigram[2]] + 1)  # If bigram not found, use unigram count as a fallback\n",
    "                score -= math.log(self.total_words + len(self.unigram_counts))  # Subtract total word count\n",
    "        return score\n",
    "\n",
    "    def spell_check(self, sentence):\n",
    "        corrected_sentence = []\n",
    "        for word in sentence:\n",
    "            if word not in self.unigram_counts:  # Check if the word is in the training data\n",
    "                candidates = self.generate_candidates(word)  # Generate candidate corrections for misspelled words\n",
    "                if candidates:\n",
    "                    corrected_word = max(candidates, key=lambda w: (self.score([w]), -edit_distance(word, w)))  # Choose the best correction based on score and edit distance\n",
    "                    corrected_sentence.append(corrected_word)\n",
    "                else:\n",
    "                    corrected_sentence.append(word)  # If no candidates found, keep the original word\n",
    "            else:\n",
    "                corrected_sentence.append(word)  # If the word is in the training data, keep it as is\n",
    "        return corrected_sentence\n",
    "\n",
    "    def generate_candidates(self, word):\n",
    "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes    = [L + R[1:] for L, R in splits if R]  # Delete one letter\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]  # Swap adjacent letters\n",
    "        replaces   = [L + c + R[1:] for L, R in splits if R for c in letters]  # Replace a letter with another\n",
    "        inserts    = [L + c + R for L, R in splits for c in letters]  # Insert a letter\n",
    "        return set(deletes + transposes + replaces + inserts)  # Return a set of unique candidate corrections\n",
    "    \n",
    "    def perplexity(self, corpus):\n",
    "        total_log_prob = 0\n",
    "        test_words = 0\n",
    "        for sentence in corpus:\n",
    "            total_log_prob += self.score(sentence)  # Accumulate the score of each sentence\n",
    "            test_words += len(sentence)  # Count the number of words in the test corpus\n",
    "        return math.exp(-total_log_prob / test_words)  # Calculate perplexity as the exponential of the average negative log probability\n",
    "\n",
    "def main():\n",
    "    nltk.download('punkt')  # Download the Punkt tokenizer for tokenization\n",
    "\n",
    "    url = \"http://norvig.com/big.txt\"  # URL of the text corpus to be used\n",
    "    response = requests.get(url)  # Send a GET request to retrieve the text corpus\n",
    "    corpus = nltk.sent_tokenize(response.text)  # Tokenize the corpus into sentences\n",
    "    corpus = [nltk.word_tokenize(sent) for sent in corpus]  # Tokenize each sentence into words\n",
    "    \n",
    "    # Split the corpus into training and test sets\n",
    "    split_point = int(len(corpus) * 0.8)  # Split point for the training and test data\n",
    "    train_corpus = corpus[:split_point]  # Training corpus\n",
    "    test_corpus = corpus[split_point:]  # Test corpus\n",
    "\n",
    "    model = TrigramLanguageModel()  # Create an instance of the TrigramLanguageModel class\n",
    "    model.train(corpus)  # Train the model using the entire corpus\n",
    "    \n",
    "    print(\"Perplexity of the model on training data: \", model.perplexity(train_corpus))  # Calculate and print the perplexity on the training data\n",
    "    print(\"Perplexity of the model on test data: \", model.perplexity(test_corpus))  # Calculate and print the perplexity on the test data\n",
    "\n",
    "    test_sentence = ['This', 'is', 'a', 'tst', 'sentnce', '.']\n",
    "    print('Original sentence: ', ' '.join(test_sentence))\n",
    "    corrected_sentence = model.spell_check(test_sentence)  # Perform spell checking on the test sentence\n",
    "    print('Corrected sentence: ', ' '.join(corrected_sentence))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
